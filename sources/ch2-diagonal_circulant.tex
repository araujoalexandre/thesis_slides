%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\orangebold{Part 1.} Compact Neural Networks with Diagonal and Circulant Matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Circulant matrices for Deep Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  Recall the Fully-Connected layer:
  \begin{equation}
    \xvec \mapsto \rho\left( {\color<2->{OrangePSL}{\Wmat}} \xvec + \bvec \right)
  \end{equation}

  \vspace{-0.4cm}
  where $\Wmat \in \Rbb^{n \times n}$, $\bvec \in \Rbb^n$.

  \vspace{0.3cm}
  \visible<2->{
    \begin{mdframed}[linecolor=OrangePSL,linewidth=1pt]
      \centering
      % \textbf{Goal}: We want to reduce the number of parameters of the layer. \\
       Can we replace the dense matrix $\Wmat$ with a structured one ? 
    \end{mdframed}
  } 
   
  \vspace{0.3cm}
  \visible<3->{
    Circulant matrices have numerous advantages:
    \begin{itemize}
	\item[$\bullet$] <3-> A circulant matrix can be \orangebold{compactly represented in memory};
	\item[$\bullet$] <3-> The matrix-vector product with a circulant matrix \orangebold{can be done efficiently in the Fourier domain}
        \item[$\Rightarrow$] <4-> They are not expressive: circulant matrices are closed under product.
    \end{itemize}
  }


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Expressivity of the product of diagonal and circulant matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{minipage}{\textwidth}
    \centering
    Combining circulant matrices with \orangebold{diagonal matrices} improve the expressivity.
  \end{minipage}
  \vspace{0.05cm}

  \visible<2->{
    \begin{theorem}[Reformulation from {\color{SkyBlue}{\citet{huhtanen2015factoring}}}] 
      For every matrix $\Amat \in \Cbb^{n \times n}$, for any $\epsilon > 0$, there exists a sequence of circulant matrices and a sequence of diagonal matrices such that 
      \begin{equation}
	\norm{\prod_{i=1}^{n+1} \Dmat^{(i)} \Cmat^{(i)} - \Amat}_{\mathrm{F}} < \epsilon \enspace.
      \end{equation}
    \end{theorem}
  }

  {\small
  \visible<3->{\textbf{Advantages}}
  \begin{itemize}[parsep=0pt,leftmargin=15pt]
    \nointerlineskip
    \item[$\bullet$] <3-> Neural networks with Diagonal and Circulant Layer are \textbf{universal approximators}; 
  \end{itemize}

  \visible<4->{\textbf{Limits}}
  \begin{itemize}[parsep=0pt,leftmargin=15pt]
    \nointerlineskip
    \item[$\bullet$] <4-> The decomposition needs more values that $n^2$
    \item[$\bullet$] <5-> The theorem does not provide any insights regarding the expressive power of $m$ diagonal-circulant factors when $m$ is much lower than $n + 1$
  \end{itemize}
  }

  % By combining this result and the universal approximation theorem of Neural Network ({\color{SkyBlue}{\cite{hanin2017universal}}}), we have the following result:
  % \begin{theorem}
  %   Bounded width DCNNs are \textbf{universal approximators} 
  % \end{theorem}


  % This theorem is of little use to understand the expressive power of diagonal-circulant matrices when they are used in deep neural networks:
  % \begin{itemize}
  %     \item The bound only depends on the dimension of the matrix $\Amat$
  %     \item The theorem does not provide any insights regarding the expressive power of $m$ diagonal-circulant factors when $m$ is much lower than $2n - 1$
  % \end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Relation between diagonal circulant matrices and low rank matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{minipage}{\textwidth}
    \textbf{Question:} Can we devise a expressivity result with a product of less than $n + 1$ diagonal-circulant factors ?
  \end{minipage}

  \visible<2->{
    \begin{theorem}[Rank-based circulant decomposition]
      For every matrix $\Amat \in \Cbb^{n \times n}$ of rank $r$, for any $\epsilon > 0$, there exists a sequence of $2r+1$ diagonal-circulant factors such that: 
      \begin{equation}
	\norm{\prod_{i=1}^{2r+1} \Dmat^{(i)} \Cmat^{(i)} - \Amat}_{\mathrm{F}} < \epsilon \enspace.
      \end{equation}
    \end{theorem}
  }

  \visible<3->{
    \textbf{Remark}: If the number of diagonal-circulant factors is set to a value $k$, we can represent all linear transform whose rank is $\frac{k - 1}{2}$.
  }

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Idea of the proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  Let $\Amat \in \Cbb^{n \times n}$ be a matrix of rank $r$ and let $\Amat = \Umat \boldsymbol{\Sigma} \Vmat^*$ be the singular value decomposition of the matrix $\Amat$
  \begin{equation*}
    \Amat = \scalebox{0.5}{\input{sources/svd.tex}}
  \end{equation*}


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Diagonal-Circulant Layer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  We replace the weight matrices of Fully-Connected layers by a product of Diagonal and Circulant matrices :

  \begin{equation}
    \xvec \mapsto \left[ \orange{ \prod_{i=1}^{k} \Dmat^{(i)} \Cmat^{(i)} } \right] \xvec + \bvec
  \end{equation}
  where
  \begin{itemize}
    \item[$\bullet$] $\xvec \in \Rbb^n$, $\bvec \in \Rbb^n$,
    \item[$\bullet$] $\Dmat^{(i)} \in \Rbb^{n \times n}$ is a diagonal matrix,
    \item[$\bullet$] $\Cmat^{(i)} \in \Rbb^{n \times n}$ is a circulant matrix,
    \item[$\bullet$] $k$ is a user defined parameter controlling the expressivity.
  \end{itemize}

  \pause
  \textbf{Remark:} Instead of defining a parameter $k$ for each Diagonal-Circulant layer, we set them all to $k = 1$ and we adjust the depth of the network.

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Expressive Power of Diagonal-Circulant Neural Network}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{theorem}[Rank-based expressive power of DCNNs]
    Let $N$ be a neural network of width $n$, depth $p$ and a sum of ranks of the weight matrices $k$.
    Then, for any $\epsilon>0$, there exists a DCNN $N'$ of width $n$ such that 
    \begin{equation}
      \norm{N(\xvec) - N'(\xvec) }_2 < \epsilon
    \end{equation}
    and the depth of $N'$ is bounded by $9k$.
  \end{theorem}

  \todo{xxx}

  Let $\Rcal_{k,n}$ be the set of all functions $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ representable by deep neural networks of total rank at most $k$ and let $\Ccal_{l,n}$ the set of all functions $f:\Rbb^{n} \rightarrow \Rbb^{n}$ representable by deep diagonal-circulant networks of depth at most $l$, then:
    \begin{align*}
      \forall k,\exists l,\forall n\, & \mathcal{R}_{k,n}\varsubsetneq\mathcal{C}_{l,n} \\
      \forall l,\nexists k,\forall n\, & \mathcal{C}_{l,n}\subseteq\mathcal{R}_{k,n}
    \end{align*}

  As we can see, the set $\Rcal_{k,n}$ of all the functions representable by a deep neural network of total rank $k$ is strictly included in the set $\Ccal_{9k}$ of all diagonal-circulant neural networks of depth $9k$. 

  \begin{figure}[htb]
    \scalebox{0.65}{\input{graphs/picture.tex}}
  \end{figure}

  
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Training of Diagonal-Circulant Neural Networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  Training diagonal circulant neural networks is hard !!

  other approaches

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Training of Diagonal-Circulant Neural Networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{block}{Large Scale Video Classification with the \yt dataset}
    \begin{itemize}
      \item 8 millions embedded audio \& video frames
      \item 3200 classes
    \end{itemize}
  \end{block}

  % The network randomly samples video and audio frames from the input. The sample goes through an embedding layer and is reduced with a Fully Connected layer. The results are then concatenated and classified with a Mixture-of-Experts and a Context Gating layer.
  State-of-the-art architecture for video classification ({\color{SkyBlue}{\cite{miech2017learnable}}}).
  \begin{figure}[htb]
    \scalebox{0.65}{\input{graphs/architecture}}
  \end{figure}
  $\Rightarrow$ This architecture has 5.7 millions parameters.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Effect of Diagonal-Circulant layers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{minipage}{\textwidth}
    \centering
    Graph representing the trade-off between accuracy and compression rate.
  \end{minipage}
  \vspace{0.2cm}

  \only<1>{
    \begin{minipage}{\textwidth}
      \centering
      \scalebox{0.8}{\input{graphs/layers_0}}
    \end{minipage}

    \vspace{0.3cm}
    \begin{minipage}{\textwidth}
      \centering
      The original architecture achieve an accuracy GAP of 84\%.
    \end{minipage}
  }

  \only<2>{
    \begin{minipage}{\textwidth}
      \centering
      \scalebox{0.8}{\input{graphs/layers_1}}
    \end{minipage}

    \vspace{0.3cm}
    \begin{minipage}{\textwidth}
      \centering
      We achieve \orangebold{9.2\% compression rate} with \orangebold{no loss} in accuracy.
    \end{minipage}
  }

  \only<3>{
    \begin{minipage}{\textwidth}
      \centering
      \scalebox{0.8}{\input{graphs/layers_2}}
    \end{minipage}

    \vspace{0.3cm}
    \begin{minipage}{\textwidth}
      \centering
      We achieve \orangebold{18\% compression rate} with a loss of \orangebold{2 points} in accuracy.
    \end{minipage}
  }

  \only<4>{
    \begin{minipage}{\textwidth}
      \centering
      \scalebox{0.8}{\input{graphs/layers_3}}
    \end{minipage}

    \vspace{0.3cm}
    \begin{minipage}{\textwidth}
      \centering
      We achieve \orangebold{72\% compression rate} with a loss of only \orangebold{4 points} in accuracy.
    \end{minipage}
  }


\end{frame}


